{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:15:23.908043Z",
     "start_time": "2021-05-12T15:15:23.891088Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import tensorflow as tf\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv') #Training Data File\n",
    "train.Summary.fillna('', inplace=True)\n",
    "train.Text.fillna('', inplace=True)\n",
    "train['summary_text'] = train['Summary'] + ' ' + train['Text']#Combining Summary and body text of review data as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPunctuations(text):\n",
    "    count=0\n",
    "    for char in text: \n",
    "        if(char in string.punctuation): count+=1\n",
    "    return count\n",
    "def toBin(row):\n",
    "    rowT=row\n",
    "    M=row.idxmax()\n",
    "    row.values[:]=0\n",
    "    row[M]=1\n",
    "    print(rowT,row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Data Analysis\n",
    "train['char_count'] = train['summary_text'].apply(len)\n",
    "temp=train['summary_text'].apply(lambda x: x.split())\n",
    "train['word_count'] = temp.apply(lambda x: len(x))\n",
    "train['punctuation_count']=train['summary_text'].apply(countPunctuations)\n",
    "temp=temp.apply(lambda x :[len(word) for word in x])\n",
    "train['max_word_length'] =temp.apply(max)\n",
    "train['min_word_length'] =temp.apply(min)\n",
    "train['avg_word_length'] =temp.apply(np.mean)\n",
    "helpfulCols=[ 'HelpfulnessNumerator','HelpfulnessDenominator', 'Score', 'Summary', 'Text', 'summary_text', 'char_count', 'word_count', 'punctuation_count',\n",
    "       'max_word_length', 'min_word_length', 'avg_word_length']\n",
    "uselessCols=['Id', 'ProductId', 'UserId', 'ProfileName', 'Time']\n",
    "trainH=train[helpfulCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n",
    "train.describe()\n",
    "trainH.groupby(train['Score']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[train[\"Score\"]==i for i in range(1,6)]\n",
    "classCounts=[(c).sum() for c in classes]\n",
    "minimum=min(classCounts)\n",
    "d = pd.DataFrame(0, index=np.arange(minimum*5), columns=[\"Score\",\"summary_text\"])\n",
    "summary_text=[(train[\"summary_text\"][c])[:minimum].values for c in classes]\n",
    "score=       [(train[\"Score\"]       [c])[:minimum].values for c in classes]\n",
    "arr,arr2=np.array([]),np.array([])\n",
    "for i in score:arr=np.append(arr,i)\n",
    "for i in summary_text:arr2=np.append(arr2,i)\n",
    "d[\"Score\"],d[\"summary_text\"]=arr,arr2\n",
    "\n",
    "# binaryCatagories\n",
    "for i in range(1,6):\n",
    "    d[\"{0}\".format(i)]=d[\"Score\"].apply(lambda x:x==i and 1 or 0)\n",
    "d=d.drop(columns=\"Score\")\n",
    "\n",
    "# trainX,testX,trainY,testY=train_test_split(d['summary_text'], d[['1','2','3','4','5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimizations\n",
    "lemmatizer = WordNetLemmatizer\n",
    "noStem=lambda x:x\n",
    "trans=str.maketrans('','',string.punctuation)#removes punctuations\n",
    "stopwordset=[word.translate(trans) for word in stopwords.words('english')]\n",
    "stemmer =nltk.stem.SnowballStemmer('english',ignore_stopwords=True)\n",
    "\n",
    "# preprocessing regexes\n",
    "stripLinks='((https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*))'#matches links \n",
    "stripSpecialChars='([^\\ a-zA-Z]+)'#matches special characters \n",
    "stripNums=\"([0-9])\"#matches numbers\n",
    "regex=stripLinks+\"|\"+stripSpecialChars+\"|\"+stripNums\n",
    "\n",
    "#functions \n",
    "def filterString(x): \n",
    "    return re.sub(regex,'',x)\n",
    "def lowerStemStopwordsRemove(x,stemmer=stemmer.stem): \n",
    "    return ' '.join([stemmer(word) for word in ((x.lower()).split()) if word not in stopwordset])\n",
    "def filterWithTensor(input_data):#slower than ^^\n",
    "    trimmed=tf.strings.strip(tf.strings.regex_replace(tf.strings.lower(input_data),regex,''))\n",
    "    return tf.compat.as_str_any(tf.strings.regex_replace(trimmed,\"(\\ {2,})\",' ').numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"summary_text\"]= d[\"summary_text\"].apply(lambda x:lowerStemStopwordsRemove(filterString(x),noStem))\n",
    "trainX,testX,trainY,testY=train_test_split(d['summary_text'], d[['1','2','3','4','5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramConfig=(1,2)    #NGram setup for the models\n",
    "vocab_size = 10000   #Vocabulary size\n",
    "sequence_length = 50 #max sentence length for the NN models \n",
    "embedding_dim=16     #Embedding layer dims\n",
    "\n",
    "# def standardize(input_data):\n",
    "#     lowercase = tf.strings.lower(input_data)\n",
    "#     filtered=tf.strings.regex_replace(lowercase,regex,'')\n",
    "#     trimmed=tf.strings.strip(filtered)\n",
    "#     return tf.strings.regex_replace(trimmed,\"(\\ {2,})\",' ')\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "#     standardize=standardize,\n",
    "    standardize=None,     #don't perform any standardization since we have already done it\n",
    "    max_tokens=vocab_size, \n",
    "    output_mode='int',    #\"int\", \"binary\", \"count\" or \"tf-idf\"\n",
    "    ngrams=ngramConfig,\n",
    "    output_sequence_length=sequence_length#output sequence length\n",
    "    )\n",
    "\n",
    "vectorize_layer.adapt(train[\"summary_text\"].tolist())\n",
    "#NN Call back configurations with early stopping implemented to avoid overfitting \n",
    "# callbacks=[tf.keras.callbacks.TensorBoard(log_dir=\"logs\") ,EarlyStopping(monitor='val_loss', min_delta=0, patience=3)]\n",
    "callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.columns=['1','2','3','4','5']\n",
    "# testY.columns=['1','2','3','4','5']\n",
    "# predictions=predictions.apply(toBin,axis=1)\n",
    "# testcompare=testY.idxmax(axis=1).reset_index().drop(columns=\"index\")\n",
    "# testcompare==predictions\n",
    "# pre=predictions.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
    "# H=model.fit(\n",
    "#    trainX, trainY,\n",
    "#     epochs=15,\n",
    "#     batch_size=50,\n",
    "#     validation_data=(testX, testY),\n",
    "#     callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(\n",
    "              input_dim=len(vectorize_layer.get_vocabulary()),\n",
    "              output_dim=embedding_dim\n",
    "            ),\n",
    "    LSTM(embedding_dim,activation='relu'),\n",
    "    Dense(embedding_dim, activation='relu'),\n",
    "    Dense(5,activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "# model.fit(\n",
    "#     trainX, trainY,\n",
    "#     epochs=15,\n",
    "#     batch_size=50,\n",
    "#     validation_data=(testX, testY),\n",
    "#     callbacks=callbacks)\n",
    "# predictions =pd.DataFrame(model.predict(testX))\n",
    "# predictions.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('wordbag', CountVectorizer(analyzer=\"word\",ngram_range=ngramConfig,stop_words=\"english\",binary=False)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', SVC(gamma='auto')),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# history = model.fit(trainX, trainY, epochs=2000, batch_size=72, verbose=1, shuffle=False)\n",
    "# predictions=model.predict(testX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
